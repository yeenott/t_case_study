{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875f753-2f86-4418-909c-11ab8cf87eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, RocCurveDisplay, recall_score, precision_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, PrecisionRecallDisplay \n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bde23-253e-4b8a-8646-6911e0ae905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Helper Functions: plot_lift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d2c95-5cad-484b-bc4b-c27d53fe179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc81476-442c-48fd-a8ee-457fdf3fea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"HIS_combined_821.csv\",index_col = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ad638-82b8-4b5d-ad58-73cdd5f09b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_to_remove = df_removed[df_removed['消费项目'].str.contains('口腔常规检查', regex=False)].reset_index()['index']\n",
    "#idx_to_remove.head()\n",
    "#print(idx_to_remove.shape, len(o))\n",
    "df_leftjoined2=df_removed.reset_index().merge(idx_to_remove, on='index', how='left', indicator=True)\n",
    "#df_leftjoined2.head()\n",
    "df_removed2 = df_leftjoined2[~(df_leftjoined2['_merge']=='both')].drop('_merge', axis=1)\n",
    "df_removed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9ae4d-de30-4ab6-8a7f-09b51154448c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_to_remove = df_removed[df_removed['收费分类'].str.contains('种植费', regex=False) & df_removed['消费项目'].str.contains('手术一次性材料费（手术时收）', regex=False)]\\\n",
    "                         .reset_index()['index']\n",
    "df_leftjoined3=df_removed2.merge(idx_to_remove, on='index', how='left', indicator=True)\n",
    "\n",
    "df_removed3 = df_leftjoined3[~(df_leftjoined3['_merge']=='both')].drop('_merge', axis=1)\n",
    "df_removed3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8102ef6-42dd-457e-b116-79842340083b",
   "metadata": {},
   "source": [
    "## 2. processing positive and negative patient dataframe\n",
    "### 2.1 positive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23428add-460f-46f9-99f1-6e11d92d0fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# series of patient_uid with implant billing \n",
    "pt_uid_implant = df_removed3[df_removed3['收费分类']==\"种植费\"]['patient_uid'].drop_duplicates()\n",
    "\n",
    "# create subset of df pt_implant_all for patient_uid with implant billing \n",
    "pt_implant_all=df_removed3.merge(pt_uid_implant, on='patient_uid', how='inner')\n",
    "\n",
    "pt_implant_all['date']=pd.to_datetime(pt_implant_all['消费时间']).dt.date\n",
    "pt_implant_all = pt_implant_all.fillna('Unknown')\n",
    "\n",
    "print(df_removed3.shape, pt_implant_all.shape)#,  pat_implant1.shape)\n",
    "pt_implant_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca58421-9ff1-4af7-b558-a6262b7acf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from the implant df subset, select the earliest date with implant billing for every patient as the implant date\n",
    "pt_implant_all.sort_values(by=['patient_uid', 'date'], ascending=True, inplace=True)\n",
    "pt_implant_date = pt_implant_all[pt_implant_all['收费分类']==\"种植费\"][['patient_uid', 'date']]\\\n",
    "                                .groupby('patient_uid').agg(implant_date=('date', min))\n",
    "\n",
    "\n",
    "# create a column 'implant date' and 'ayearbfore_implant_date' for every patient \n",
    "pt_implant_all1=pt_implant_all.set_index('patient_uid')\n",
    "for pt in pt_uid_implant:\n",
    "    pt_implant_all1.loc[pt, 'implant_date']=pt_implant_date.loc[pt, 'implant_date']\n",
    "    \n",
    "pt_implant_all1['a_year_bfore_implant_date'] = pt_implant_all1['implant_date']-datetime.timedelta(days= 365) \n",
    "\n",
    "\n",
    "# filter pt records within specified time window, \n",
    "# that is, dates within a year prior to the implant date and implant date is at least a year before the start date 2018/01/01 \n",
    "pt_implant_all2 = pt_implant_all1[(pt_implant_all1['date'] < pt_implant_all1['implant_date'])\\\n",
    "                & (pt_implant_all1['date'] >= pt_implant_all1['a_year_bfore_implant_date'])\\\n",
    "                & (pt_implant_all1['a_year_bfore_implant_date'] >= pd.to_datetime('2018-01-01 00:00')) ].drop('index', axis=1)\n",
    "\n",
    "print(pt_implant_all1.shape, pt_implant_date, pt_implant_all2.shape)\n",
    "pt_implant_all2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e6ccb-ad97-4497-8fe9-fccfe3c8f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unique pt: before time window \"{}\", after \"{}\"\\n------------------------'.format(pt_uid_implant.shape[0], len(pt_implant_all2.index.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca5d17-8925-497a-85a4-a541b5be28f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_feature_df(df, groupby_field='patient_uid'):\n",
    "\n",
    "    f1 = df.groupby('patient_uid').agg(diagnosis=('诊断名称', 'unique'))\n",
    "    f2 = df.groupby('patient_uid').agg(treatment_plan=('治疗计划', 'unique'))\n",
    "    f3 = df.groupby('patient_uid').agg(purchase=('消费项目', 'unique'))\n",
    "    f4 = df.groupby('patient_uid').agg(total_order=('项目金额（实付）', 'sum'))\n",
    "    f5 = df.groupby('patient_uid').agg(dept=('科室', 'unique'))\n",
    "    f6 = df.groupby('patient_uid').agg(gender=('性别', 'unique'))\n",
    "    f7 = df.groupby('patient_uid').agg(age=('年龄', 'max'))\n",
    "    f8 = df.groupby('patient_uid').agg(dxcode=('诊断代码', 'unique'))\n",
    "    f9 = df.groupby('patient_uid').agg(order_type=('收费分类', 'unique'))\n",
    "\n",
    "    f10 = df.groupby(['patient_uid', 'date']).agg(order_by_visit=('项目金额（实付）', 'sum'))\\\n",
    "                        .groupby('patient_uid').agg(mean_order=('order_by_visit', 'mean'))\n",
    "\n",
    "    f11 = df.groupby('patient_uid').agg(date_list=('date', 'unique'))\n",
    "    f11['duration_days'] = f11['date_list'].map(lambda x: (x.max()-x.min()).days)\n",
    "    \n",
    "    f12 = df.groupby('patient_uid').agg(visits=('date', 'nunique'))\n",
    "    \n",
    "    \n",
    "    f_list = [f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12]\n",
    "    linked_df = reduce(lambda x, y: x.merge(y, on='patient_uid', how='inner'), f_list)\n",
    "    linked_df['visit_interval']=linked_df['duration_days']/linked_df['visits']\n",
    "    return linked_df\n",
    "\n",
    "linked_implant_df = create_feature_df(pt_implant_all2)\n",
    "linked_implant_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b565a-c2bd-4282-b216-bb0c36477563",
   "metadata": {},
   "source": [
    "### 2.2. Negative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1d9ae-3d18-4fcd-94ba-49c8a13707d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed4 = df_removed3.merge(pt_uid_implant, on='patient_uid', how='left', indicator=True)\n",
    "\n",
    "pt_noimplant_all = df_removed4[df_removed4['_merge']=='left_only'].drop('_merge', axis=1)\n",
    "\n",
    "pt_noimplant_all['date']=pd.to_datetime(pt_noimplant_all['消费时间']).dt.date\n",
    "pt_noimplant_all = pt_noimplant_all.fillna('Unknown')\n",
    "\n",
    "pt_noimplant_all_1=pt_noimplant_all.set_index('patient_uid')\n",
    "pt_uid_noimplant = pt_noimplant_all_1.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a61f6-380b-467a-88e8-086b75d69ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from the noimplant pt subset, select the latest date for every patient, apply a time windeow to filter records within a year from the latest date\n",
    "pt_noimplant_lastest_date = pt_noimplant_all.groupby('patient_uid').agg(latest_date=('date', 'max'))\n",
    "\n",
    "# create a column 'implant date' and 'ayearbfore_implant_date' for every patient \n",
    "for pt in pt_uid_noimplant:\n",
    "    pt_noimplant_all_1.loc[pt, 'latest_date']=pt_noimplant_lastest_date.loc[pt, 'latest_date']\n",
    "    \n",
    "pt_noimplant_all_1['a_year_bfore_latest_date'] = pt_noimplant_all_1['latest_date']-datetime.timedelta(days= 365) \n",
    "# pt_noimplant_all_1.head()\n",
    "\n",
    "pt_noimplant_all_2=pt_noimplant_all_1[ (pt_noimplant_all_1['date'] > pt_noimplant_all_1['a_year_bfore_latest_date'])\\\n",
    "                   & (pt_noimplant_all_1['date'] < pt_noimplant_all_1['latest_date'])\n",
    "                   & (pt_noimplant_all_1['a_year_bfore_latest_date'] > pd.to_datetime('2018-01-01 00:00')) ].drop('index', axis=1).drop_duplicates()\n",
    "\n",
    "#print(pt_noimplant_all_1.shape, pt_noimplant_lastest_date, pt_noimplant_all_2.shape)\n",
    "#print(pre_noimplant.shape, pt_noimplant_all_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdb86c-06d0-4c6e-abf8-45844b804d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linked_noimplant_df = create_feature_df(pt_noimplant_all_2)\n",
    "linked_noimplant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f605f48-97e5-4dcb-98f5-f17253e40ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe of implant patient from YH {}'.format(linked_implant_df.shape))#, link_implant.shape))\n",
    "print('Dataframe of non-implant patient from YH {}'.format(linked_noimplant_df.shape))#, link_noimplant.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd10ae7-f6f2-456a-9866-bd43954ff4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linked_noimplant_df['implant']=0\n",
    "linked_implant_df['implant']=1\n",
    "\n",
    "linked_all = linked_noimplant_df.append(linked_implant_df)\n",
    "linked_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a00337-4114-43b0-ac90-71b0a52c0d1a",
   "metadata": {},
   "source": [
    "## 3. EDA, Data Cleaning, Built Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96060414-aab4-4e0c-8b5b-687f238dcf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455e3fb-799c-4a17-88f0-e84afff7a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_all[linked_all['implant']==1].describe()\n",
    "linked_all[linked_all['implant']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826917b-9c8f-4b62-b348-43e52f0efff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.boxplot([linked_all[linked_all['implant']==1]['total_order'], linked_all[linked_all['implant']==0]['total_order']]);\n",
    "\n",
    "# ax = linked_all[linked_all['implant']==0]['total_order'].plot.hist(density=True)\n",
    "\n",
    "def plt_compare_hist(df, label, col_name, bins=50, density=True):\n",
    "    \n",
    "    _, bins, _ = plt.hist(df[df[label]==0][col_name], bins=bins, density=density, label='negative')#, normed=True)\n",
    "    _ = plt.hist(linked_all[linked_all[label]==1][col_name], bins=bins, alpha=0.5, density=True, label='positive')#, normed=True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show();\n",
    "    return\n",
    "\n",
    "def create_compare_violin(df, col_name, label='implant'):\n",
    "    sns.violinplot(data=df, x=label, y=col_name, hue=label, palette=\"muted\", split=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204e3de-029a-42d8-bb1f-b8ed02905710",
   "metadata": {},
   "source": [
    "### 3.1. EDA, remove outlier in \"total_order\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794f25b-8cbe-48c2-997c-8bdccb270966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare_hist(linked_all, 'implant', 'total_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f5a04-142f-45d7-9021-876b3b01fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linked_all[(linked_all['implant']==0) & (linked_all['total_order']>50000)].head()\n",
    "#linked_all[(linked_all['implant']==1) & (linked_all['total_order']>50000)].head()\n",
    "linked_all1=linked_all[~(linked_all['total_order']>40000)]\n",
    "print(linked_all.shape, linked_all1.shape)\n",
    "plt_compare_hist(linked_all1, 'implant', 'total_order')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa2629-32b9-4617-a68c-7d83e4ec115b",
   "metadata": {},
   "source": [
    "### 3.2. EDA of mean_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328bfe1b-7259-4b38-8bb5-50fa9494e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare_hist(linked_all1, 'implant', 'mean_order')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd39ec2-842d-4cf0-aa9e-bed33bcf72cb",
   "metadata": {},
   "source": [
    "### 3.3. EDA, Cleaning of \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35494e04-dbe8-4d99-8911-34dcf5c47302",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_all2=linked_all1[~(linked_all1['age']<10)]\n",
    "plt_compare_hist(linked_all2, 'implant', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493210-0161-41f6-af17-a4cd5ca0eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare_hist(linked_all2, 'implant', 'duration_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6744bf4-3f02-4a7d-951a-52aa6469dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare_hist(linked_all2, 'implant', 'visits', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d6fba-3a4f-4b3c-8e04-df4642f391f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare_hist(linked_all2, 'implant', 'visit_interval', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d77c5-e7da-4871-845f-589c0ea64ce2",
   "metadata": {},
   "source": [
    "### 3.4. Cleaning, Contingency Table for \"gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054dfeaa-ebcc-475f-9728-be8d1b528af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove pt with mulitple gender\n",
    "linked_all3 = linked_all2[~(linked_all2['gender'].map(lambda x: len(x)) > 1)].copy()\n",
    "\n",
    "linked_all3.loc[:, 'gender'] = linked_all3.loc[:,'gender'].map(lambda x: x[0]).map({'女': 0, '男': 1})\n",
    "linked_all3['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a049e7-b3cd-4f56-bdc3-863ee587d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = linked_all3.reset_index().groupby(['implant', 'gender']).agg(cnt=('patient_uid', 'count'))\n",
    "\n",
    "crosstab = linked_all3.reset_index().pivot_table(index='gender', columns='implant', values='patient_uid', aggfunc='count', margins=True)\n",
    "crosstab\n",
    "crosstab1=crosstab.copy()\n",
    "crosstab1.loc[:,0:1]=crosstab1.loc[:,0:1].div(crosstab1['All'], axis=0)\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b583f-3047-4de3-af7c-a136af897b6b",
   "metadata": {},
   "source": [
    "### 3.5. Cleaning of \"diagnosis\" and creating a patient diagnosis sparse matrix, truncate low frequency diagnosis < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608d6a2-0624-47a1-80b8-97a6101caa27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleaned_text(d: list):\n",
    "    return [o for o in re.sub(r'[^\\u4e00-\\u9fff]', ' ', str(d)).split(\" \") if (o != \"\") & (len(o) > 0) ]\n",
    "\n",
    "def cleaned_code(d: list):\n",
    "    return [x for x in  re.findall(r'[a-zA-Z]\\d{2}.\\d{3}', str(d))]\n",
    "\n",
    "def create_pt_categorical_feature_matrix(df0, col, truncated_n=0):\n",
    "    \n",
    "    df=df0.copy()\n",
    "    # clean text in col\n",
    "    l = []\n",
    "    for d in df[col]:    \n",
    "        l += cleaned_text(d)\n",
    "    \n",
    "    # create a corpus of categorical features from col\n",
    "    bag_of_cat_feature = set(l)\n",
    "    \n",
    "    df.loc[:, col] = df.loc[:, col].map(cleaned_text)\n",
    "    \n",
    "    # create a dict of {feature: count of feature}\n",
    "    cnt_feature=dict()\n",
    "    for d in bag_of_cat_feature:\n",
    "        v = 0\n",
    "        for m in l:\n",
    "            if m==d:\n",
    "                v+=1\n",
    "        cnt_feature[d]=v\n",
    "    \n",
    "    if truncated_n>0:\n",
    "        cnt_feature_sorted = {k: v for k,v in sorted(cnt_feature.items(), key=lambda item: item[1], reverse=True) if v>truncated_n}\n",
    "    else:\n",
    "        cnt_feature_sorted = {k: v for k,v in sorted(cnt_feature.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # iterate on pt_uid and col to create  {pt_uid: one-hot coding of categorical feature dict}\n",
    "    bag_of_featuer_sorted = set([k for k in cnt_feature_sorted.keys()])\n",
    "    \n",
    "    # create a {patient_uid: feature coding dict}\n",
    "    pt_feature_dict=dict()  \n",
    "    for pt in df.index:\n",
    "        pt_feature=df.loc[pt, col]\n",
    "\n",
    "        # Iterating bag_of_featuer_sorted, if feature in \"col\" = 1, else = 0 \n",
    "        feature_matrix = dict()\n",
    "        for d in list(bag_of_featuer_sorted):\n",
    "            if d in pt_feature:\n",
    "                feature_matrix[d]=1\n",
    "            else:\n",
    "                feature_matrix[d]=0\n",
    "\n",
    "        pt_feature_dict[pt]=feature_matrix\n",
    "    \n",
    "    # create a pt_uid, categorical feature dataframe and rename columns\n",
    "    df_pt_feature = pd.DataFrame.from_dict(pt_feature_dict, orient='index')\n",
    "    \n",
    "    cols_feature_dict = {v: col+str(i) for i, v in enumerate(cnt_feature_sorted)}\n",
    "\n",
    "    df_pt_feature.rename(columns=cols_feature_dict, inplace=True)\n",
    "    \n",
    "    return df_pt_feature, cols_feature_dict, cnt_feature_sorted\n",
    "\n",
    "\n",
    "def create_pt_vector(df0, col, vector_type, truncated_n, is_dxcode=False):\n",
    "    \n",
    "    df=df0.copy()\n",
    "    # clean text in col\n",
    "    l = []\n",
    "    \n",
    "    if is_dxcode:\n",
    "        for d in df[col]:    \n",
    "            l += cleaned_code(d)\n",
    "        bag_of_cat_feature = set(l)\n",
    "        df.loc[:, col] = df.loc[:, col].map(cleaned_code).astype(str)\n",
    "    \n",
    "    else:\n",
    "        for d in df[col]:    \n",
    "            l += cleaned_text(d)\n",
    "    # create a corpus of categorical features from col\n",
    "        bag_of_cat_feature = set(l)\n",
    "        df.loc[:, col] = df.loc[:, col].map(cleaned_text).astype(str)\n",
    "    \n",
    "    \n",
    "    if vector_type == 'tf':\n",
    "        # using count-Vector \n",
    "        if is_dxcode:\n",
    "            token_pattern_dxcode=r'[a-zA-Z]\\d{2}.\\d{3}'\n",
    "            vectorizer = CountVectorizer(min_df=truncated_n, token_pattern=token_pattern_dxcode)\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(min_df=truncated_n)  \n",
    "        X = vectorizer.fit_transform(df[col])\n",
    "        bag_of_features = vectorizer.get_feature_names() \n",
    "        count_of_features = X.toarray().sum(axis=0)\n",
    "        \n",
    "        # using tf-idf vectorizer \n",
    "    elif vector_type =='tfidf':\n",
    "        if is_dxcode:\n",
    "            token_pattern_dxcode=r'[a-zA-Z]\\d{2}.\\d{3}'\n",
    "            vectorizer = TfidfVectorizer(min_df=truncated_n, token_pattern=token_pattern_dxcode)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(min_df=truncated_n)\n",
    "            \n",
    "        X = vectorizer.fit_transform(df[col])\n",
    "        bag_of_features = vectorizer.get_feature_names() \n",
    "        count_of_features = vectorizer.idf_\n",
    "        \n",
    "    cnt_feature = dict(zip(bag_of_features, count_of_features))\n",
    "    feature_vector_sorted = {k: v for k,v in sorted(cnt_feature.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    cols_feature_map = {v: col+str(i) for i, v in enumerate(feature_vector_sorted)}\n",
    "    feature_cols_map = {v:k for k, v in cols_feature_map.items() }\n",
    "    df1 = pd.DataFrame(X.toarray(), columns=bag_of_features, index=df0.index)\n",
    "\n",
    "    out_df = df1.rename(columns=cols_feature_map)\n",
    "        \n",
    "    return out_df, feature_vector_sorted, feature_cols_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d5b7b-b24e-4b45-b476-8db72abda366",
   "metadata": {},
   "source": [
    "### 3.6. Create a \"dx\" feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f804ab-42e1-40fa-a941-255f77d9e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_dx, dx_vector_dict, dx_col_map= create_pt_vector(linked_all3, 'diagnosis', 'tfidf', 10)\n",
    "len(dx_vector_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98ba3653-9927-432c-b6e1-7c3c730a4ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_pt_dx, cols_dx_dict, cnt_dx_feature_sorted= create_pt_categorical_feature_matrix(linked_all3, 'diagnosis', 20)\n",
    "df_pt_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56903d05-a64f-415c-845a-fa7916da1dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pt_dx['implant']=linked_all3['implant']\n",
    "corr = df_pt_dx.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, vmax=1.0, vmin=-1.0, cmap=cmap)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dba3d-9d7d-4498-9818-3bd73099509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_diagnosis = corr[corr['implant'].abs()<0.005].index\n",
    "{d:dx_col_map[d] for d in drop_diagnosis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71ecf2-fc3a-4ae4-ad4a-9dbcdc4619de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "implant_pt_dx = df_pt_dx[df_pt_dx['implant']==1].sum().drop('implant').to_dict()\n",
    "temp_implant = pd.DataFrame().from_dict(implant_pt_dx, orient='index', columns=['implant_pt_cnt']).sort_values(by=['implant_pt_cnt'], ascending=False)\n",
    "temp_implant.head(10)\n",
    "\n",
    "noimplant_pt_dx = df_pt_dx[df_pt_dx['implant']==0].sum().to_dict()\n",
    "temp_noimplant = pd.DataFrame.from_dict(noimplant_pt_dx, orient='index', columns=['noimplant_pt_cnt']).sort_values(by=['noimplant_pt_cnt'], ascending=False)\n",
    "temp_noimplant.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7566841-ac4c-4a54-ae47-0af37e9016a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_feature_reduction(df, cols, pca_alias, label_col, n_components=2):\n",
    "    \n",
    "    x = df.loc[:, cols].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    y = df.loc[:, label_col].values\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(x)\n",
    "    print('cumulative varation: {}\\nExplained variation per principal component: {}'.format(pca.explained_variance_ratio_.sum(), pca.explained_variance_ratio_))\n",
    "    \n",
    "    xi = np.arange(1, n_components+1, step=1)\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "    \n",
    "    pca_cols = [pca_alias+'_pca_'+str(k) for k in range(n_components)]\n",
    "    pca_df = pd.DataFrame(data=pca_components, columns=pca_cols, index=df.index)\n",
    "    \n",
    "    #final_pca_df=pca_df.merge(df[[label_col]].reset_index(), on='index', how='left').set_index('index')\n",
    "    final_pca_df=pd.concat([pca_df, df[[label_col]]], axis=1)#.merge(df[[label_col]].reset_index(), on='index', how='left').set_index('index')\n",
    "    \n",
    "    # scatter plot of 2-D pca\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    targets = [0, 1]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = final_pca_df[label_col] == target\n",
    "        ax.scatter(final_pca_df.loc[indicesToKeep, pca_alias+'_pca_0'], final_pca_df.loc[indicesToKeep, pca_alias+'_pca_1'], c=color, s=20, alpha=0.3);\n",
    "    ax.legend(targets);\n",
    "    #ax.grid();\n",
    "    \n",
    "    return final_pca_df#, pca_components.explained_variance_ratio_\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_marker(df, cols, alias, label_col, n=2):\n",
    "    \n",
    "    x = df.loc[:, cols].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    y = df.loc[:, label_col].values\n",
    "   \n",
    "\n",
    "    #pca = PCA(n_components=m)\n",
    "    tsne = TSNE(n_components=n)#, perplexity=20, n_iter=1000, learning_rate=200)\n",
    "    #tsne_after_pca = Pipeline([('pca', pca), ('tsne', tsne)])\n",
    "    dr = tsne.fit_transform(x)\n",
    "   \n",
    "    tsne_cols = [alias+'_tsne_'+str(k) for k in range(n_components)]\n",
    "    tsne_df = pd.DataFrame(data=dr, columns=tsne_cols, index=df.index)\n",
    "    final_tsne_df=pd.concat([tsne_df, df[[label_col]]], axis=1)\n",
    "    \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    targets = [0, 1]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = final_tsne_df[label_col] == target\n",
    "        ax.scatter(final_tsne_df.loc[indicesToKeep, alias+'_tsne_0'], final_tsne_df.loc[indicesToKeep, alias+'_tsne_1'], c=color, s=20, alpha=0.3);\n",
    "    ax.legend(targets);\n",
    "    \n",
    "    return tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b2c78-dad5-4df5-b52d-cc6a9838f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_pt_dx.columns if 'diagnosis' in c]\n",
    "df_pt_dx['implant']=linked_all3['implant']\n",
    "\n",
    "dx_pca_df = pca_feature_reduction(df_pt_dx, cols, 'dx', 'implant', 20)\n",
    "dx_pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba1893-69c4-4ac6-8eb4-5b2b294470a2",
   "metadata": {},
   "source": [
    "### 3.7. Create a \"dxcode\" feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3521d-95d7-454a-8874-96cf3babf087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_code=True\n",
    "df_pt_dxcode, dxcode_vector_dict, dxcode_col_map= create_pt_vector(linked_all3, 'dxcode', 'tfidf', 10, is_code)\n",
    "len(dxcode_vector_dict)\n",
    "dxcode_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff70deb-e4c7-4538-a2e7-c7cdbd96a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pt_dxcode['implant']=linked_all3['implant']\n",
    "corr = df_pt_dxcode.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, vmax=1.0, vmin=-1.0, cmap=cmap)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3ac96-4d48-47e0-8aea-b0558fd39a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr['implant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a07fb1-9e0e-447e-a44a-7e5b7990d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dxcode = corr[corr['implant'].abs()<0.005].index\n",
    "{d: dxcode_col_map[d] for d in drop_dxcode}\n",
    "df_pt_dxcode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bf7ad-3a7f-486e-897a-28bdfb99c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_dxcode.drop(drop_dxcode, axis=1, inplace=True)\n",
    "df_pt_dxcode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0997c1-6335-4cd9-a79b-e1ad7d17d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_pt_dxcode.columns if 'dxcode' in c]\n",
    "df_pt_dxcode['implant']=linked_all3['implant']\n",
    "\n",
    "dxcode_pca_df = pca_feature_reduction(df_pt_dxcode, cols, 'dxcode', 'implant', 20)\n",
    "dxcode_pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38480a-7de4-4772-b3e7-73d6860e68a2",
   "metadata": {},
   "source": [
    "### 3.8. Create a \"dept\" feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c21d-19b1-409d-a956-2a0522566b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_dept, cols_dept_dict, cnt_dept_feature_sorted= create_pt_categorical_feature_matrix(linked_all3, 'dept')\n",
    "df_pt_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7195d-a9e5-4c94-854c-fa26b42304e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_dept, dept_vector_dict, dept_col_map = create_pt_vector(linked_all3, 'dept', 'tfidf', 1)\n",
    "len(dept_vector_dict)\n",
    "#df_pt_dx, dx_vector_dict, dx_col_map = create_pt_vector(linked_all3, 'diagnosis', 'tf', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a50a2e-0a93-483b-b70c-a8b1e26e71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea6a2a-493d-4f81-a670-6bb1d57e0a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pt_dept['implant']=linked_all3['implant']\n",
    "corr = df_pt_dept.corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, vmax=1.0, vmin=-1, cmap=cmap)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1539b-26a2-44f6-8986-b37f445dc3be",
   "metadata": {},
   "source": [
    "### 3.9. Create a px feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c0bf1-42a5-4fae-be01-deacfbe0dc24",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_pt_px, cols_px_dict, cnt_px_feature_sorted= create_pt_categorical_feature_matrix(linked_all3, 'purchase', 20)\n",
    "cnt_px_feature_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cce403-d0c1-4c89-8a06-caaafe4307f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_px, px_vector_dict, px_col_map= create_pt_vector(linked_all3, 'purchase', 'tf', 20)\n",
    "len(px_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ddb3f-cb87-4d7e-babb-cf70b8eece0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pt_px['implant']=linked_all3['implant']\n",
    "corr = df_pt_px.corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, vmax=1.0, vmin=-1, cmap=cmap)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46f8ec-f731-48dd-9e04-4edaa92b55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr['implant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd2547-f6f0-42c6-a10e-9ec06d84b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_px = corr[corr['implant'].abs()<0.005].index\n",
    "{d: px_col_map[d] for d in drop_px}\n",
    "df_pt_px.drop(drop_px, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683cfbe-96f5-4051-babe-84429e682bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_pt_px.columns if 'purchase' in c]\n",
    "df_pt_px['implant']=linked_all3['implant']\n",
    "\n",
    "px_pca_df = pca_feature_reduction(df_pt_px, cols, 'px', 'implant', 40)\n",
    "px_pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897c640-015b-48f0-972d-2834759b2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_marker(df, cols, alias, label_col, n=2):\n",
    "    \n",
    "    x = df.loc[:, cols].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    y = df.loc[:, label_col].values\n",
    "   \n",
    "\n",
    "    #pca = PCA(n_components=m)\n",
    "    tsne = TSNE(n_components=n)#, perplexity=20, n_iter=1000, learning_rate=200)\n",
    "    #tsne_after_pca = Pipeline([('pca', pca), ('tsne', tsne)])\n",
    "    dr = tsne.fit_transform(x)\n",
    "   \n",
    "    tsne_cols = [alias+'_tsne_'+str(k) for k in range(n)]\n",
    "    tsne_df = pd.DataFrame(data=dr, columns=tsne_cols, index=df.index)\n",
    "    final_tsne_df=pd.concat([tsne_df, df[[label_col]]], axis=1)\n",
    "    \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    targets = [0, 1]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = final_tsne_df[label_col] == target\n",
    "        ax.scatter(final_tsne_df.loc[indicesToKeep, alias+'_tsne_0'], final_tsne_df.loc[indicesToKeep, alias+'_tsne_1'], c=color, s=20, alpha=0.3);\n",
    "    ax.legend(targets);\n",
    "    \n",
    "    return tsne_df\n",
    "\n",
    "cols = [c for c in df_pt_px.columns if 'purchase' in c]\n",
    "px_tsne_df = tsne_marker(df_pt_px, cols, 'px', 'implant', 2)\n",
    "px_tsne_df.head()\n",
    "#x_train = tsne_marker(train_x, l, n_pca, n_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5911b2a-ea31-4d94-99e4-48a4164efae6",
   "metadata": {},
   "source": [
    "### 3.10 Create final feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384969a-57db-48e6-a1b5-d0b971de96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = linked_all3[['total_order', 'age', 'mean_order', 'visits', 'duration_days', 'implant']].corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, vmax=1.0, vmin=-1, cmap=cmap, annot=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0ccbd-7068-415b-af79-23279689796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pt_dx.shape, px_pca_df.shape, df_pt_dept.shape, dxcode_pca_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca140700-7aca-4781-b77f-381797c12c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_df = pd.concat([linked_all3, \n",
    "                       df_pt_dx.drop(['implant'], axis=1), \n",
    "                       df_pt_dept.drop(['implant'], axis=1), \n",
    "                       px_pca_df.drop(['implant'], axis=1), \n",
    "                       dxcode_pca_df.drop(['implant'], axis=1)], axis=1)\n",
    "\n",
    "linked_df.drop(['diagnosis', 'treatment_plan', 'purchase', 'order_type', 'date_list', 'mean_order', 'dept', 'dxcode'], axis=1, inplace=True)\n",
    "linked_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be670ba-8bfd-4221-b190-801fd939b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5e88b-104c-40dd-ad35-9d126237fbc4",
   "metadata": {},
   "source": [
    "## 4. Train Model\n",
    "### 4.1. Train-test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f33594-3505-457c-806c-6683b2058054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = linked_df.drop(axis = 1, columns = ['implant']).values\n",
    "Y0 = linked_df['implant'].values\n",
    "print(X0.shape, Y0.shape)\n",
    "pd.Series(Y0).value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e97421-abcd-47b2-88b2-6480ab5a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(train_x0, train_y0, sampling):\n",
    "    \n",
    "    if sampling == 'under':\n",
    "        nm = NearMiss(version=3)\n",
    "        train_x, train_y = nm.fit_resample(train_x0, train_y0)\n",
    "    elif sampling =='over':\n",
    "        oversample = BorderlineSMOTE(random_state=10, kind=\"borderline-2\")\n",
    "        train_x, train_y = oversample.fit_resample(train_x0, train_y0.ravel())\n",
    "    pd.Series(train_y).value_counts().plot.bar();\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "train_x0, test_x0, train_y0, test_y0 = train_test_split(X0, Y0, test_size = 0.3, random_state = 1)\n",
    "\n",
    "train_x, train_y = sampling(train_x0, train_y0, 'under')\n",
    "train_x = StandardScaler().fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55393c35-2a64-4b88-8b37-05c5c8405087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_x, test_y = sampling(test_x0, test_y0, 'under')\n",
    "test_x = StandardScaler().fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e598d1a-5a36-44d3-bb9f-0e02dcce8e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "nm = NearMiss(version=3)\n",
    "X, Y = nm.fit_resample(X0, Y0)\n",
    "#pd.Series(Y).value_counts().plot.bar();\n",
    "\n",
    "train_x, test_x ,train_y, test_y = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "pd.Series(train_y).value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae38c39-5a41-44ba-8284-fa08353f47bc",
   "metadata": {},
   "source": [
    "## Helper Fuctions: plot_lift_curve, plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de941eea-884f-42a6-b6a8-3578e7141232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lift_curve(y_val, y_pred, step=0.01):\n",
    "    '''\n",
    "    y_val: label of test data \n",
    "    y_pred: probability of prediction ons for such data\n",
    "    step: resolution\n",
    "    '''    \n",
    "    aux_lift = pd.DataFrame()\n",
    "    aux_lift['real'] = y_val\n",
    "    aux_lift['predicted'] = y_pred\n",
    "    # sorted by the predicted probability column:\n",
    "    aux_lift.sort_values('predicted',ascending=False,inplace=True)\n",
    "    \n",
    "    x_val = np.arange(step, 1+step, step)\n",
    "    # Ratio of positive label\n",
    "    ratio_ones = aux_lift['real'].sum() / len(aux_lift)\n",
    "    \n",
    "    y_v = []\n",
    "    for x in x_val:\n",
    "        num_data = int(np.ceil(x*len(aux_lift))) # the # of data points based on ratio x \n",
    "        data_here = aux_lift.iloc[:num_data,:]  # a subset of sorted data\n",
    "        ratio_ones_here = data_here['real'].sum()/len(data_here) # ratio of positive labels within this subset\n",
    "        y_v.append(ratio_ones_here / ratio_ones)\n",
    "           \n",
    "   #Plot the figure\n",
    "    fig, axis = plt.subplots()\n",
    "    axis.plot(x_val, y_v, 'r-');\n",
    "    axis.plot(x_val, np.ones(len(x_val)), 'g-');\n",
    "    axis.set_xlabel('Proportion of sample');\n",
    "    axis.set_ylabel('Lift');\n",
    "    plt.show();\n",
    "\n",
    "    return\n",
    "\n",
    "def plot_learning_curve(model, train_x, train_y, test_x, test_y, n_splits=10):\n",
    "    '''\n",
    "    train_x, train_y: training data and training label\n",
    "    test_x, test_y: training data and training label\n",
    "    '''    \n",
    "    X = np.concatenate((train_x, test_x), axis=0)\n",
    "    Y = np.concatenate((test_y, train_y), axis=0)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state=7) \n",
    "    \n",
    "    LC = learning_curve(estimator=model, X=X, y=Y, cv=kfold, train_sizes=np.linspace(0.10, 1.00, 20))\n",
    "\n",
    "    #plt.figure(figsize =(12, 10))\n",
    "    plt.plot(LC[0],np.nanmean(LC[1], axis=1),c='blue');\n",
    "    plt.plot(LC[0],np.nanmean(LC[2], axis=1), c='green');\n",
    "    plt.show();\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6037e09-5998-466f-bcf4-8c913571238b",
   "metadata": {},
   "source": [
    "### 4.2. Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779417e-e3dd-4b6f-abe8-6f3c2d925a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(oob_score = True).fit(train_x, train_y)\n",
    "prob_rfc = rfc.predict_proba(test_x)\n",
    "class_rfc = rfc.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, class_rfc))\n",
    "print('AUC: ', roc_auc_score(test_y, prob_rfc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ffd60-4171-44c4-bb6a-736cbd4945e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = prob_rfc[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48597ae-9945-4662-b81c-edaa598b03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recall, _ = precision_recall_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c4c67-247b-4668-8396-2610279f2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "X1 = np.concatenate((train_x, test_x), axis=0)\n",
    "Y1 = np.concatenate((test_y, train_y), axis=0)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle = True, random_state=7) \n",
    "\n",
    "LC = learning_curve(estimator=rfc, X=X1, y=Y1, cv=kfold, train_sizes=np.linspace(0.10, 1.00, 10))\n",
    "\n",
    "#plt.figure(figsize =(12, 10))\n",
    "plt.plot(LC[0],np.nanmean(LC[1], axis=1), c='blue');\n",
    "plt.plot(LC[0],np.nanmean(LC[2], axis=1), c='green');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1139645-dfee-4c29-a27d-7a3a180f3133",
   "metadata": {},
   "source": [
    "### 4.3. Recursivce Feature Elimination (RFECV) in feature selecting to avoid over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a5d82-4e94-4806-b120-2b13e2b4e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=rfc, step=3, cv=StratifiedKFold(10), scoring='recall')\n",
    "rfecv.fit(train_x, train_y)\n",
    "\n",
    "print('The optimal number of features: {}'.format(rfecv.n_features_))\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_,)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b830ee7-add0-4eab-a54b-5f6830824888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_list = linked_df.drop(axis = 1, columns = ['implant']).columns\n",
    "col_idx = {i: v for i, v in enumerate(feature_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58b9bc-c7c4-4875-ba68-3394a3071e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = linked_df.drop(axis = 1, columns = ['implant']).columns\n",
    "\n",
    "selected_feature_list=[feature_list[i] for i, v in enumerate(rfecv.support_ ) if v == True]\n",
    "\n",
    "selected_features = dict(zip(selected_feature_list, rfecv.estimator_.feature_importances_))\n",
    "\n",
    "df_selected_features = pd.DataFrame().from_dict(selected_features, orient='index', columns=['importance']).sort_values(by=['importance'], ascending=False)\n",
    "df_selected_features.head(10)\n",
    "\n",
    "df_selected_features.plot.barh(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e1748-e783-4a5e-8e8e-eb5498be6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2fa8a-6d1a-481d-bd9f-f152c4317478",
   "metadata": {},
   "source": [
    "### 4.4. Re-train model with selected featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd64c8f-f1ed-4c18-a8bf-c3b8d65bbd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(train_x0, train_y0, sampling):\n",
    "    \n",
    "    if sampling == 'under':\n",
    "        nm = NearMiss(version=3)\n",
    "        train_x, train_y = nm.fit_resample(train_x0, train_y0)\n",
    "    elif sampling =='over':\n",
    "        oversample = BorderlineSMOTE(random_state=10, kind=\"borderline-2\")\n",
    "        train_x, train_y = oversample.fit_resample(train_x0, train_y0.ravel())\n",
    "    pd.Series(train_y).value_counts().plot.bar();\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "X1 = linked_df.drop(axis = 1, columns = ['implant'])[selected_feature_list].values\n",
    "Y1 = linked_df['implant'].values\n",
    "\n",
    "train_x0, test_x0, train_y0, test_y0 = train_test_split(X1, Y1, test_size = 0.3, random_state = 1)\n",
    "\n",
    "train_x, train_y = sampling(train_x0, train_y0, 'over')\n",
    "train_x = StandardScaler().fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2809d-e167-4d06-aa85-8748386bb5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_x, test_y = sampling(test_x0, test_y0, 'over')\n",
    "test_x = StandardScaler().fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd8943-dd2a-4a71-9905-7e2bd1ca98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(oob_score = True).fit(train_x, train_y)\n",
    "prob_out = rfc.predict_proba(test_x)\n",
    "class_out = rfc.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, class_out))\n",
    "print('AUC: ', roc_auc_score(test_y, prob_out[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f7dbb-6a25-47e3-a99c-f57060721e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probablity output of the positive \n",
    "print(rfc.classes_)\n",
    "y_score = prob_out[:, 1]\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "# precision-recall curve\n",
    "prec, recall, _ = precision_recall_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()\n",
    "\n",
    "# lift-curve\n",
    "plot_lift_curve(test_y, y_score, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a59eca-5a3f-468c-a5ef-9fffd15cbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "plot_learning_curve(rfc2, train_x, train_y, test_x, test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725558ab-c8e6-44cf-9304-e53b43d38e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354bcd7-9afe-4a19-8ff2-2818da294521",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.arange(10, 100, 10)\n",
    "min_samples_leaf = np.arange(1, 4, 2)\n",
    "bootstrap=[True, False]\n",
    "\n",
    "param_grid = dict(min_samples_leaf=min_samples_leaf, max_depth=max_depth, bootstrap=bootstrap)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "grid_search = GridSearchCV(rfc2 ,param_grid=param_grid, scoring='recall', n_jobs=-1, cv=kfold)\n",
    "grid_result = gridsearch.fit(train_x, train_y) \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be0f85-f8ed-486c-b4d0-d8cc0a72ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, gridsearch.best_params_))\n",
    "\n",
    "best_grid = gridsearch.best_estimator_\n",
    "\n",
    "prob_out = best_grid.predict_proba(test_x)\n",
    "class_out = best_grid.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, class_out))\n",
    "print('AUC: ', roc_auc_score(test_y, prob_out[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda11264-f42a-4580-beb8-926605085230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probablity output of the positive \n",
    "#print(rfc.classes_)\n",
    "y_score = prob_out[:, 1]\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "# precision-recall curve\n",
    "prec, recall, _ = precision_recall_curve(test_y, y_score, pos_label=rfc.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()\n",
    "\n",
    "# lift-curve\n",
    "plot_lift_curve(test_y, y_score, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1d405-c57c-4fed-96a8-6761e52c598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "plot_learning_curve(best_grid, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a905140-d13e-4147-b6b9-908974c3e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "XGB = XGBClassifier(objective = \"binary:logistic\", tree_method='auto')\n",
    "XGB.fit(train_x, train_y, eval_set = [(test_x, test_y)], verbose = 0)\n",
    "\n",
    "prob_out = XGB.predict_proba(test_x)\n",
    "class_out = XGB.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, class_out))\n",
    "roc_auc_score(test_y, prob_out[:,1])\n",
    "\n",
    "y_score = prob_out[:, 1]\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(test_y, y_score, pos_label=XGB.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "# precision-recall curve\n",
    "prec, recall, _ = precision_recall_curve(test_y, y_score, pos_label=XGB.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd0d37-c456-45b1-a968-45abc268b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importance_plot(model, metric = \"gain\"):\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(12, 10))\n",
    "    plot_importance(model, height=0.8, ax=ax, max_num_features=100, importance_type = metric)\n",
    "\n",
    "    id_l = [int(re.findall(r\"\\d+\",str(i))[2]) for i in plt.yticks()[1]]\n",
    "    id_l.reverse()\n",
    "    f_name = [selected_feature_list[i] for i in id_l]\n",
    "    plt.yticks(range(len(f_name)),f_name,fontsize = 10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "f_importance_plot(XGB, metric ='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775368c-af7f-4cc3-95c7-1c5c8c322169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
